import re
from typing import Dict, List, Optional
from urllib.parse import urlparse, parse_qs, unquote

from models.data_models import LinkInfo
from utils.logger import logger
from .url_resolver import URLResolver

class LinkAnalyzer:
    """Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§"""
    
    def __init__(self):
        self.redirect_mapping: Dict[str, str] = {}
        self.url_resolver: Optional[URLResolver] = None
    
    async def initialize_resolver(self):
        """Ø´Ø±ÙˆØ¹ Ú©Ø±Ø¯Ù† URL resolver"""
        self.url_resolver = URLResolver(timeout=10, max_redirects=5)
        await self.url_resolver.__aenter__()
    
    async def cleanup_resolver(self):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ URL resolver"""
        if self.url_resolver:
            await self.url_resolver.__aexit__(None, None, None)
    
    async def resolve_links_with_http(self, links: List[str]) -> List[str]:
        """Ø­Ù„ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² HTTP requests"""
        if not self.url_resolver:
            await self.initialize_resolver()
        
        resolved_links = []
        logger.info(f"ğŸ” Resolving {len(links)} links with HTTP requests...")
        
        # Ø­Ù„ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        resolved_urls = await self.url_resolver.resolve_multiple_urls(links)
        
        for original_link, resolved_link in resolved_urls.items():
            if resolved_link and self.url_resolver.is_telegram_link(resolved_link):
                logger.info(f"âœ… Resolved to Telegram: {original_link[:50]}... -> {resolved_link}")
                self.redirect_mapping[original_link] = resolved_link
                resolved_links.append(resolved_link)
            elif resolved_link:
                logger.warning(f"âš ï¸ Resolved but not Telegram: {original_link[:50]}... -> {resolved_link}")
                # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ø­Ù„ Ø´Ø¯ ÙˆÙ„ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… Ù†ÛŒØ³ØªØŒ Ø¢Ù† Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            else:
                logger.error(f"âŒ Could not resolve: {original_link[:50]}...")
                # Ø§Ú¯Ø± Ù†ØªÙˆØ§Ù†Ø³ØªÛŒÙ… Ø­Ù„ Ú©Ù†ÛŒÙ…ØŒ Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
                resolved_links.append(original_link)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
        stats = self.url_resolver.get_redirect_stats()
        logger.info(f"ğŸ“Š URL Resolution Stats:")
        logger.info(f"   â€¢ Total resolved: {stats['total_resolved']}")
        logger.info(f"   â€¢ Telegram redirects: {stats['telegram_redirects']}")
        logger.info(f"   â€¢ Non-Telegram redirects: {stats['non_telegram_redirects']}")
        logger.info(f"   â€¢ Failed redirects: {stats['failed_redirects']}")
        
        return resolved_links
    
    def extract_telegram_link_from_redirect(self, redirect_url: str) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© ØªÙ„Ú¯Ø±Ø§Ù… Ø§ØµÙ„ÛŒ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø±ÛŒØ¯Ø§ÛŒØ±Ú©Øª (legacy method)"""
        try:
            # Google Translate links
            if 'translate.google.com' in redirect_url:
                parsed = urlparse(redirect_url)
                params = parse_qs(parsed.query)
                
                if 'u' in params:
                    original_url = unquote(params['u'][0])
                    if 't.me' in original_url:
                        return original_url
            
            # Ø³Ø§ÛŒØ± Ø§Ù†ÙˆØ§Ø¹ Ø±ÛŒØ¯Ø§ÛŒØ±Ú©Øªâ€ŒÙ‡Ø§
            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ø§Ù†ÙˆØ§Ø¹ Ø¯ÛŒÚ¯Ø± Ø±ÛŒØ¯Ø§ÛŒØ±Ú©Øª Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
            
            return None
        except Exception as e:
            logger.error(f"Error extracting from redirect {redirect_url}: {e}")
            return None
    
    async def resolve_redirect_links(self, links: List[str]) -> List[str]:
        """Ø­Ù„ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø±ÛŒØ¯Ø§ÛŒØ±Ú©Øª Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ (updated method)"""
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Ø¬Ø¯ÛŒØ¯ HTTP-based resolution
        return await self.resolve_links_with_http(links)
    
    def categorize_telegram_link(self, link: str) -> LinkInfo:
        """Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…"""
        link_info = LinkInfo(
            original_link=link,
            type="unknown",
            identifier="",
            is_invite_link=False,
            is_redirect=False
        )
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø±ÛŒØ¯Ø§ÛŒØ±Ú©Øª
        if link in self.redirect_mapping:
            link_info.is_redirect = True
            link_info.redirect_source = link
            link = self.redirect_mapping[link]  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ
        
        if not 't.me' in link:
            link_info.type = "invalid"
            return link_info
        
        try:
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù„ÛŒÙ†Ú©
            clean_link = link.split('?')[0]  # Ø­Ø°Ù query parameters
            
            if '/joinchat/' in clean_link:
                # Ù„ÛŒÙ†Ú© Ø¯Ø¹ÙˆØª
                link_info.type = "invite_link"
                link_info.is_invite_link = True
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ invite hash
                hash_part = clean_link.split('/joinchat/')[-1]
                link_info.identifier = hash_part
                
            elif '/+' in clean_link:
                # ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯ Ù„ÛŒÙ†Ú© Ø¯Ø¹ÙˆØª
                link_info.type = "invite_link"
                link_info.is_invite_link = True
                hash_part = clean_link.split('/+')[-1]
                link_info.identifier = hash_part
                
            else:
                # Ù„ÛŒÙ†Ú© Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø§ username
                link_info.type = "public_link"
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ username
                username = clean_link.replace("https://t.me/", "").replace("@", "")
                if '/' in username:
                    username = username.split('/')[0]
                link_info.identifier = username
                
        except Exception as e:
            logger.error(f"Error categorizing link {link}: {e}")
            link_info.type = "invalid"
        
        return link_info
    
    def extract_links_from_text(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù…ØªÙ†"""
        if not text:
            return []
            
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù„ÛŒÙ†Ú©
        patterns = [
            r'https?://[^\s<>"\']+',
            r'www\.[^\s<>"\']+',
            r't\.me/[^\s<>"\']+',
            r'@[a-zA-Z0-9_]+',
        ]
        
        links = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            links.extend(matches)
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        cleaned_links = []
        for link in links:
            # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø§Ø² Ø§Ù†ØªÙ‡Ø§
            link = re.sub(r'[.,;:!?)\\]}]+$', '', link)
            if link:
                cleaned_links.append(link)
        
        return cleaned_links
